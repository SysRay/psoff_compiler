#pragma once
#include "ir/config.h"

namespace compiler::frontend::parser {

enum class eEncoding {
  SOP1,
  SOP2,
  SOPP,
  SOPC,
  EXP,
  VINTRP,
  DS,
  MUBUF,
  MTBUF,
  MIMG,
  SMRD,
  SOPK,
  VOP1,
  VOP2,
  VOP3,
  VOPC,
  Custom,
  UNKNOWN,
};

enum class eOpcode : InstructionKind_t {
  UNKNOWN,
  DS_ADD_U32              = 1,
  DS_SUB_U32              = 1 + DS_ADD_U32,
  DS_RSUB_U32             = 2 + DS_ADD_U32,
  DS_INC_U32              = 3 + DS_ADD_U32,
  DS_DEC_U32              = 4 + DS_ADD_U32,
  DS_MIN_I32              = 5 + DS_ADD_U32,
  DS_MAX_I32              = 6 + DS_ADD_U32,
  DS_MIN_U32              = 7 + DS_ADD_U32,
  DS_MAX_U32              = 8 + DS_ADD_U32,
  DS_AND_B32              = 9 + DS_ADD_U32,
  DS_OR_B32               = 10 + DS_ADD_U32,
  DS_XOR_B32              = 11 + DS_ADD_U32,
  DS_MSKOR_B32            = 12 + DS_ADD_U32,
  DS_WRITE_B32            = 13 + DS_ADD_U32,
  DS_WRITE2_B32           = 14 + DS_ADD_U32,
  DS_WRITE2ST64_B32       = 15 + DS_ADD_U32,
  DS_CMPST_B32            = 16 + DS_ADD_U32,
  DS_CMPST_F32            = 17 + DS_ADD_U32,
  DS_MIN_F32              = 18 + DS_ADD_U32,
  DS_MAX_F32              = 19 + DS_ADD_U32,
  DS_NOP                  = 20 + DS_ADD_U32,
  DS_GWS_SEMA_RELEASE_ALL = 24 + DS_ADD_U32,
  DS_GWS_INIT             = 25 + DS_ADD_U32,
  DS_GWS_SEMA_V           = 26 + DS_ADD_U32,
  DS_GWS_SEMA_BR          = 27 + DS_ADD_U32,
  DS_GWS_SEMA_P           = 28 + DS_ADD_U32,
  DS_GWS_BARRIER          = 29 + DS_ADD_U32,
  DS_WRITE_B8             = 30 + DS_ADD_U32,
  DS_WRITE_B16            = 31 + DS_ADD_U32,
  DS_ADD_RTN_U32          = 32 + DS_ADD_U32,
  DS_SUB_RTN_U32          = 33 + DS_ADD_U32,
  DS_RSUB_RTN_U32         = 34 + DS_ADD_U32,
  DS_INC_RTN_U32          = 35 + DS_ADD_U32,
  DS_DEC_RTN_U32          = 36 + DS_ADD_U32,
  DS_MIN_RTN_I32          = 37 + DS_ADD_U32,
  DS_MAX_RTN_I32          = 38 + DS_ADD_U32,
  DS_MIN_RTN_U32          = 39 + DS_ADD_U32,
  DS_MAX_RTN_U32          = 40 + DS_ADD_U32,
  DS_AND_RTN_B32          = 41 + DS_ADD_U32,
  DS_OR_RTN_B32           = 42 + DS_ADD_U32,
  DS_XOR_RTN_B32          = 43 + DS_ADD_U32,
  DS_MSKOR_RTN_B32        = 44 + DS_ADD_U32,
  DS_WRXCHG_RTN_B32       = 45 + DS_ADD_U32,
  DS_WRXCHG2_RTN_B32      = 46 + DS_ADD_U32,
  DS_WRXCHG2ST64_RTN_B32  = 47 + DS_ADD_U32,
  DS_CMPST_RTN_B32        = 48 + DS_ADD_U32,
  DS_CMPST_RTN_F32        = 49 + DS_ADD_U32,
  DS_MIN_RTN_F32          = 50 + DS_ADD_U32,
  DS_MAX_RTN_F32          = 51 + DS_ADD_U32,
  DS_WRAP_RTN_B32         = 52 + DS_ADD_U32,
  DS_SWIZZLE_B32          = 53 + DS_ADD_U32,
  DS_READ_B32             = 54 + DS_ADD_U32,
  DS_READ2_B32            = 55 + DS_ADD_U32,
  DS_READ2ST64_B32        = 56 + DS_ADD_U32,
  DS_READ_I8              = 57 + DS_ADD_U32,
  DS_READ_U8              = 58 + DS_ADD_U32,
  DS_READ_I16             = 59 + DS_ADD_U32,
  DS_READ_U16             = 60 + DS_ADD_U32,
  DS_CONSUME              = 61 + DS_ADD_U32,
  DS_APPEND               = 62 + DS_ADD_U32,
  DS_ORDERED_COUNT        = 63 + DS_ADD_U32,
  DS_ADD_U64              = 64 + DS_ADD_U32,
  DS_SUB_U64              = 65 + DS_ADD_U32,
  DS_RSUB_U64             = 66 + DS_ADD_U32,
  DS_INC_U64              = 67 + DS_ADD_U32,
  DS_DEC_U64              = 68 + DS_ADD_U32,
  DS_MIN_I64              = 69 + DS_ADD_U32,
  DS_MAX_I64              = 70 + DS_ADD_U32,
  DS_MIN_U64              = 71 + DS_ADD_U32,
  DS_MAX_U64              = 72 + DS_ADD_U32,
  DS_AND_B64              = 73 + DS_ADD_U32,
  DS_OR_B64               = 74 + DS_ADD_U32,
  DS_XOR_B64              = 75 + DS_ADD_U32,
  DS_MSKOR_B64            = 76 + DS_ADD_U32,
  DS_WRITE_B64            = 77 + DS_ADD_U32,
  DS_WRITE2_B64           = 78 + DS_ADD_U32,
  DS_WRITE2ST64_B64       = 79 + DS_ADD_U32,
  DS_CMPST_B64            = 80 + DS_ADD_U32,
  DS_CMPST_F64            = 81 + DS_ADD_U32,
  DS_MIN_F64              = 82 + DS_ADD_U32,
  DS_MAX_F64              = 83 + DS_ADD_U32,
  DS_ADD_RTN_U64          = 96 + DS_ADD_U32,
  DS_SUB_RTN_U64          = 97 + DS_ADD_U32,
  DS_RSUB_RTN_U64         = 98 + DS_ADD_U32,
  DS_INC_RTN_U64          = 99 + DS_ADD_U32,
  DS_DEC_RTN_U64          = 100 + DS_ADD_U32,
  DS_MIN_RTN_I64          = 101 + DS_ADD_U32,
  DS_MAX_RTN_I64          = 102 + DS_ADD_U32,
  DS_MIN_RTN_U64          = 103 + DS_ADD_U32,
  DS_MAX_RTN_U64          = 104 + DS_ADD_U32,
  DS_AND_RTN_B64          = 105 + DS_ADD_U32,
  DS_OR_RTN_B64           = 106 + DS_ADD_U32,
  DS_XOR_RTN_B64          = 107 + DS_ADD_U32,
  DS_MSKOR_RTN_B64        = 108 + DS_ADD_U32,
  DS_WRXCHG_RTN_B64       = 109 + DS_ADD_U32,
  DS_WRXCHG2_RTN_B64      = 110 + DS_ADD_U32,
  DS_WRXCHG2ST64_RTN_B64  = 111 + DS_ADD_U32,
  DS_CMPST_RTN_B64        = 112 + DS_ADD_U32,
  DS_CMPST_RTN_F64        = 113 + DS_ADD_U32,
  DS_MIN_RTN_F64          = 114 + DS_ADD_U32,
  DS_MAX_RTN_F64          = 115 + DS_ADD_U32,
  DS_READ_B64             = 118 + DS_ADD_U32,
  DS_READ2_B64            = 119 + DS_ADD_U32,
  DS_READ2ST64_B64        = 120 + DS_ADD_U32,
  DS_CONDXCHG32_RTN_B64   = 126 + DS_ADD_U32,
  DS_ADD_SRC2_U32         = 128 + DS_ADD_U32,
  DS_SUB_SRC2_U32         = 129 + DS_ADD_U32,
  DS_RSUB_SRC2_U32        = 130 + DS_ADD_U32,
  DS_INC_SRC2_U32         = 131 + DS_ADD_U32,
  DS_DEC_SRC2_U32         = 132 + DS_ADD_U32,
  DS_MIN_SRC2_I32         = 133 + DS_ADD_U32,
  DS_MAX_SRC2_I32         = 134 + DS_ADD_U32,
  DS_MIN_SRC2_U32         = 135 + DS_ADD_U32,
  DS_MAX_SRC2_U32         = 136 + DS_ADD_U32,
  DS_AND_SRC2_B32         = 137 + DS_ADD_U32,
  DS_OR_SRC2_B32          = 138 + DS_ADD_U32,
  DS_XOR_SRC2_B32         = 139 + DS_ADD_U32,
  DS_WRITE_SRC2_B32       = 141 + DS_ADD_U32,
  DS_MIN_SRC2_F32         = 146 + DS_ADD_U32,
  DS_MAX_SRC2_F32         = 147 + DS_ADD_U32,
  DS_ADD_SRC2_U64         = 192 + DS_ADD_U32,
  DS_SUB_SRC2_U64         = 193 + DS_ADD_U32,
  DS_RSUB_SRC2_U64        = 194 + DS_ADD_U32,
  DS_INC_SRC2_U64         = 195 + DS_ADD_U32,
  DS_DEC_SRC2_U64         = 196 + DS_ADD_U32,
  DS_MIN_SRC2_I64         = 197 + DS_ADD_U32,
  DS_MAX_SRC2_I64         = 198 + DS_ADD_U32,
  DS_MIN_SRC2_U64         = 199 + DS_ADD_U32,
  DS_MAX_SRC2_U64         = 200 + DS_ADD_U32,
  DS_AND_SRC2_B64         = 201 + DS_ADD_U32,
  DS_OR_SRC2_B64          = 202 + DS_ADD_U32,
  DS_XOR_SRC2_B64         = 203 + DS_ADD_U32,
  DS_WRITE_SRC2_B64       = 205 + DS_ADD_U32,
  DS_MIN_SRC2_F64         = 210 + DS_ADD_U32,
  DS_MAX_SRC2_F64         = 211 + DS_ADD_U32,
  DS_WRITE_B96            = 222 + DS_ADD_U32,
  DS_WRITE_B128           = 223 + DS_ADD_U32,
  DS_CONDXCHG32_RTN_B128  = 253 + DS_ADD_U32,
  DS_READ_B96             = 254 + DS_ADD_U32,
  DS_READ_B128            = 255 + DS_ADD_U32,

  // EXP
  EXP,

  // MIMG
  IMAGE_LOAD,
  IMAGE_LOAD_MIP         = 1 + IMAGE_LOAD,
  IMAGE_LOAD_PCK         = 2 + IMAGE_LOAD,
  IMAGE_LOAD_PCK_SGN     = 3 + IMAGE_LOAD,
  IMAGE_LOAD_MIP_PCK     = 4 + IMAGE_LOAD,
  IMAGE_LOAD_MIP_PCK_SGN = 5 + IMAGE_LOAD,
  IMAGE_STORE            = 8 + IMAGE_LOAD,
  IMAGE_STORE_MIP        = 9 + IMAGE_LOAD,
  IMAGE_STORE_PCK        = 10 + IMAGE_LOAD,
  IMAGE_STORE_MIP_PCK    = 11 + IMAGE_LOAD,
  IMAGE_GET_RESINFO      = 14 + IMAGE_LOAD,
  IMAGE_ATOMIC_SWAP      = 15 + IMAGE_LOAD,
  IMAGE_ATOMIC_CMPSWAP   = 16 + IMAGE_LOAD,
  IMAGE_ATOMIC_ADD       = 17 + IMAGE_LOAD,
  IMAGE_ATOMIC_SUB       = 18 + IMAGE_LOAD,
  IMAGE_ATOMIC_SMIN      = 20 + IMAGE_LOAD,
  IMAGE_ATOMIC_UMIN      = 21 + IMAGE_LOAD,
  IMAGE_ATOMIC_SMAX      = 22 + IMAGE_LOAD,
  IMAGE_ATOMIC_UMAX      = 23 + IMAGE_LOAD,
  IMAGE_ATOMIC_AND       = 24 + IMAGE_LOAD,
  IMAGE_ATOMIC_OR        = 25 + IMAGE_LOAD,
  IMAGE_ATOMIC_XOR       = 26 + IMAGE_LOAD,
  IMAGE_ATOMIC_INC       = 27 + IMAGE_LOAD,
  IMAGE_ATOMIC_DEC       = 28 + IMAGE_LOAD,
  IMAGE_ATOMIC_FCMPSWAP  = 29 + IMAGE_LOAD,
  IMAGE_ATOMIC_FMIN      = 30 + IMAGE_LOAD,
  IMAGE_ATOMIC_FMAX      = 31 + IMAGE_LOAD,
  IMAGE_SAMPLE           = 32 + IMAGE_LOAD,
  IMAGE_SAMPLE_CL        = 33 + IMAGE_LOAD,
  IMAGE_SAMPLE_D         = 34 + IMAGE_LOAD,
  IMAGE_SAMPLE_D_CL      = 35 + IMAGE_LOAD,
  IMAGE_SAMPLE_L         = 36 + IMAGE_LOAD,
  IMAGE_SAMPLE_B         = 37 + IMAGE_LOAD,
  IMAGE_SAMPLE_B_CL      = 38 + IMAGE_LOAD,
  IMAGE_SAMPLE_LZ        = 39 + IMAGE_LOAD,
  IMAGE_SAMPLE_C         = 40 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_CL      = 41 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_D       = 42 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_D_CL    = 43 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_L       = 44 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_B       = 45 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_B_CL    = 46 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_LZ      = 47 + IMAGE_LOAD,
  IMAGE_SAMPLE_O         = 48 + IMAGE_LOAD,
  IMAGE_SAMPLE_CL_O      = 49 + IMAGE_LOAD,
  IMAGE_SAMPLE_D_O       = 50 + IMAGE_LOAD,
  IMAGE_SAMPLE_D_CL_O    = 51 + IMAGE_LOAD,
  IMAGE_SAMPLE_L_O       = 52 + IMAGE_LOAD,
  IMAGE_SAMPLE_B_O       = 53 + IMAGE_LOAD,
  IMAGE_SAMPLE_B_CL_O    = 54 + IMAGE_LOAD,
  IMAGE_SAMPLE_LZ_O      = 55 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_O       = 56 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_CL_O    = 57 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_D_O     = 58 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_D_CL_O  = 59 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_L_O     = 60 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_B_O     = 61 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_B_CL_O  = 62 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_LZ_O    = 63 + IMAGE_LOAD,
  IMAGE_GATHER4          = 64 + IMAGE_LOAD,
  IMAGE_GATHER4_CL       = 65 + IMAGE_LOAD,
  IMAGE_GATHER4_L        = 68 + IMAGE_LOAD,
  IMAGE_GATHER4_B        = 69 + IMAGE_LOAD,
  IMAGE_GATHER4_B_CL     = 70 + IMAGE_LOAD,
  IMAGE_GATHER4_LZ       = 71 + IMAGE_LOAD,
  IMAGE_GATHER4_C        = 72 + IMAGE_LOAD,
  IMAGE_GATHER4_C_CL     = 73 + IMAGE_LOAD,
  IMAGE_GATHER4_C_L      = 76 + IMAGE_LOAD,
  IMAGE_GATHER4_C_B      = 77 + IMAGE_LOAD,
  IMAGE_GATHER4_C_B_CL   = 78 + IMAGE_LOAD,
  IMAGE_GATHER4_C_LZ     = 79 + IMAGE_LOAD,
  IMAGE_GATHER4_O        = 80 + IMAGE_LOAD,
  IMAGE_GATHER4_CL_O     = 81 + IMAGE_LOAD,
  IMAGE_GATHER4_L_O      = 84 + IMAGE_LOAD,
  IMAGE_GATHER4_B_O      = 85 + IMAGE_LOAD,
  IMAGE_GATHER4_B_CL_O   = 86 + IMAGE_LOAD,
  IMAGE_GATHER4_LZ_O     = 87 + IMAGE_LOAD,
  IMAGE_GATHER4_C_O      = 88 + IMAGE_LOAD,
  IMAGE_GATHER4_C_CL_O   = 89 + IMAGE_LOAD,
  IMAGE_GATHER4_C_L_O    = 92 + IMAGE_LOAD,
  IMAGE_GATHER4_C_B_O    = 93 + IMAGE_LOAD,
  IMAGE_GATHER4_C_B_CL_O = 94 + IMAGE_LOAD,
  IMAGE_GATHER4_C_LZ_O   = 95 + IMAGE_LOAD,
  IMAGE_GET_LOD          = 96 + IMAGE_LOAD,
  IMAGE_SAMPLE_CD        = 104 + IMAGE_LOAD,
  IMAGE_SAMPLE_CD_CL     = 105 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_CD      = 106 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_CD_CL   = 107 + IMAGE_LOAD,
  IMAGE_SAMPLE_CD_O      = 108 + IMAGE_LOAD,
  IMAGE_SAMPLE_CD_CL_O   = 109 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_CD_O    = 110 + IMAGE_LOAD,
  IMAGE_SAMPLE_C_CD_CL_O = 111 + IMAGE_LOAD,

  // MTBUF
  TBUFFER_LOAD_FORMAT_X,
  TBUFFER_LOAD_FORMAT_XY    = 1 + TBUFFER_LOAD_FORMAT_X,
  TBUFFER_LOAD_FORMAT_XYZ   = 2 + TBUFFER_LOAD_FORMAT_X,
  TBUFFER_LOAD_FORMAT_XYZW  = 3 + TBUFFER_LOAD_FORMAT_X,
  TBUFFER_STORE_FORMAT_X    = 4 + TBUFFER_LOAD_FORMAT_X,
  TBUFFER_STORE_FORMAT_XY   = 5 + TBUFFER_LOAD_FORMAT_X,
  TBUFFER_STORE_FORMAT_XYZ  = 6 + TBUFFER_LOAD_FORMAT_X,
  TBUFFER_STORE_FORMAT_XYZW = 7 + TBUFFER_LOAD_FORMAT_X,

  // MUBUF
  BUFFER_LOAD_FORMAT_X,
  BUFFER_LOAD_FORMAT_XY     = 1 + BUFFER_LOAD_FORMAT_X,
  BUFFER_LOAD_FORMAT_XYZ    = 2 + BUFFER_LOAD_FORMAT_X,
  BUFFER_LOAD_FORMAT_XYZW   = 3 + BUFFER_LOAD_FORMAT_X,
  BUFFER_STORE_FORMAT_X     = 4 + BUFFER_LOAD_FORMAT_X,
  BUFFER_STORE_FORMAT_XY    = 5 + BUFFER_LOAD_FORMAT_X,
  BUFFER_STORE_FORMAT_XYZ   = 6 + BUFFER_LOAD_FORMAT_X,
  BUFFER_STORE_FORMAT_XYZW  = 7 + BUFFER_LOAD_FORMAT_X,
  BUFFER_LOAD_UBYTE         = 8 + BUFFER_LOAD_FORMAT_X,
  BUFFER_LOAD_SBYTE         = 9 + BUFFER_LOAD_FORMAT_X,
  BUFFER_LOAD_USHORT        = 10 + BUFFER_LOAD_FORMAT_X,
  BUFFER_LOAD_SSHORT        = 11 + BUFFER_LOAD_FORMAT_X,
  BUFFER_LOAD_DWORD         = 12 + BUFFER_LOAD_FORMAT_X,
  BUFFER_LOAD_DWORDX2       = 13 + BUFFER_LOAD_FORMAT_X,
  BUFFER_LOAD_DWORDX4       = 14 + BUFFER_LOAD_FORMAT_X,
  BUFFER_LOAD_DWORDX3       = 15 + BUFFER_LOAD_FORMAT_X,
  BUFFER_STORE_BYTE         = 24 + BUFFER_LOAD_FORMAT_X,
  BUFFER_STORE_SHORT        = 26 + BUFFER_LOAD_FORMAT_X,
  BUFFER_STORE_DWORD        = 28 + BUFFER_LOAD_FORMAT_X,
  BUFFER_STORE_DWORDX2      = 29 + BUFFER_LOAD_FORMAT_X,
  BUFFER_STORE_DWORDX4      = 30 + BUFFER_LOAD_FORMAT_X,
  BUFFER_STORE_DWORDX3      = 31 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_SWAP        = 48 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_CMPSWAP     = 49 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_ADD         = 50 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_SUB         = 51 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_SMIN        = 53 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_UMIN        = 54 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_SMAX        = 55 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_UMAX        = 56 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_AND         = 57 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_OR          = 58 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_XOR         = 59 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_INC         = 60 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_DEC         = 61 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_FCMPSWAP    = 62 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_FMIN        = 63 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_FMAX        = 64 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_SWAP_X2     = 80 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_CMPSWAP_X2  = 81 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_ADD_X2      = 82 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_SUB_X2      = 83 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_SMIN_X2     = 85 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_UMIN_X2     = 86 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_SMAX_X2     = 87 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_UMAX_X2     = 88 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_AND_X2      = 89 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_OR_X2       = 90 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_XOR_X2      = 91 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_INC_X2      = 92 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_DEC_X2      = 93 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_FCMPSWAP_X2 = 94 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_FMIN_X2     = 95 + BUFFER_LOAD_FORMAT_X,
  BUFFER_ATOMIC_FMAX_X2     = 96 + BUFFER_LOAD_FORMAT_X,
  BUFFER_WBINVL1_SC         = 112 + BUFFER_LOAD_FORMAT_X,
  BUFFER_WBINVL1            = 113 + BUFFER_LOAD_FORMAT_X,

  // SMRD
  S_LOAD_DWORD,
  S_LOAD_DWORDX2         = 1 + S_LOAD_DWORD,
  S_LOAD_DWORDX4         = 2 + S_LOAD_DWORD,
  S_LOAD_DWORDX8         = 3 + S_LOAD_DWORD,
  S_LOAD_DWORDX16        = 4 + S_LOAD_DWORD,
  S_BUFFER_LOAD_DWORD    = 8 + S_LOAD_DWORD,
  S_BUFFER_LOAD_DWORDX2  = 9 + S_LOAD_DWORD,
  S_BUFFER_LOAD_DWORDX4  = 10 + S_LOAD_DWORD,
  S_BUFFER_LOAD_DWORDX8  = 11 + S_LOAD_DWORD,
  S_BUFFER_LOAD_DWORDX16 = 12 + S_LOAD_DWORD,
  S_DCACHE_INV_VOL       = 29 + S_LOAD_DWORD,
  S_MEMTIME              = 30 + S_LOAD_DWORD,
  S_DCACHE_INV           = 31 + S_LOAD_DWORD,

  // SOP1
  S_DUMMY_SOP1,
  S_MOV_B32            = 3 + S_DUMMY_SOP1,
  S_MOV_B64            = 4 + S_DUMMY_SOP1,
  S_CMOV_B32           = 5 + S_DUMMY_SOP1,
  S_CMOV_B64           = 6 + S_DUMMY_SOP1,
  S_NOT_B32            = 7 + S_DUMMY_SOP1,
  S_NOT_B64            = 8 + S_DUMMY_SOP1,
  S_WQM_B32            = 9 + S_DUMMY_SOP1,
  S_WQM_B64            = 10 + S_DUMMY_SOP1,
  S_BREV_B32           = 11 + S_DUMMY_SOP1,
  S_BREV_B64           = 12 + S_DUMMY_SOP1,
  S_BCNT0_I32_B32      = 13 + S_DUMMY_SOP1,
  S_BCNT0_I32_B64      = 14 + S_DUMMY_SOP1,
  S_BCNT1_I32_B32      = 15 + S_DUMMY_SOP1,
  S_BCNT1_I32_B64      = 16 + S_DUMMY_SOP1,
  S_FF0_I32_B32        = 17 + S_DUMMY_SOP1,
  S_FF0_I32_B64        = 18 + S_DUMMY_SOP1,
  S_FF1_I32_B32        = 19 + S_DUMMY_SOP1,
  S_FF1_I32_B64        = 20 + S_DUMMY_SOP1,
  S_FLBIT_I32_B32      = 21 + S_DUMMY_SOP1,
  S_FLBIT_I32_B64      = 22 + S_DUMMY_SOP1,
  S_FLBIT_I32          = 23 + S_DUMMY_SOP1,
  S_FLBIT_I32_I64      = 24 + S_DUMMY_SOP1,
  S_SEXT_I32_I8        = 25 + S_DUMMY_SOP1,
  S_SEXT_I32_I16       = 26 + S_DUMMY_SOP1,
  S_BITSET0_B32        = 27 + S_DUMMY_SOP1,
  S_BITSET0_B64        = 28 + S_DUMMY_SOP1,
  S_BITSET1_B32        = 29 + S_DUMMY_SOP1,
  S_BITSET1_B64        = 30 + S_DUMMY_SOP1,
  S_GETPC_B64          = 31 + S_DUMMY_SOP1,
  S_SETPC_B64          = 32 + S_DUMMY_SOP1,
  S_SWAPPC_B64         = 33 + S_DUMMY_SOP1,
  S_RFE_B64            = 34 + S_DUMMY_SOP1,
  S_AND_SAVEEXEC_B64   = 36 + S_DUMMY_SOP1,
  S_OR_SAVEEXEC_B64    = 37 + S_DUMMY_SOP1,
  S_XOR_SAVEEXEC_B64   = 38 + S_DUMMY_SOP1,
  S_ANDN2_SAVEEXEC_B64 = 39 + S_DUMMY_SOP1,
  S_ORN2_SAVEEXEC_B64  = 40 + S_DUMMY_SOP1,
  S_NAND_SAVEEXEC_B64  = 41 + S_DUMMY_SOP1,
  S_NOR_SAVEEXEC_B64   = 42 + S_DUMMY_SOP1,
  S_XNOR_SAVEEXEC_B64  = 43 + S_DUMMY_SOP1,
  S_QUADMASK_B32       = 44 + S_DUMMY_SOP1,
  S_QUADMASK_B64       = 45 + S_DUMMY_SOP1,
  S_MOVRELS_B32        = 46 + S_DUMMY_SOP1,
  S_MOVRELS_B64        = 47 + S_DUMMY_SOP1,
  S_MOVRELD_B32        = 48 + S_DUMMY_SOP1,
  S_MOVRELD_B64        = 49 + S_DUMMY_SOP1,
  S_CBRANCH_JOIN       = 50 + S_DUMMY_SOP1,
  S_MOV_REGRD_B32      = 51 + S_DUMMY_SOP1,
  S_ABS_I32            = 52 + S_DUMMY_SOP1,
  S_MOV_FED_B32        = 53 + S_DUMMY_SOP1,

  // SOP2
  S_ADD_U32,
  S_SUB_U32        = 1 + S_ADD_U32,
  S_ADD_I32        = 2 + S_ADD_U32,
  S_SUB_I32        = 3 + S_ADD_U32,
  S_ADDC_U32       = 4 + S_ADD_U32,
  S_SUBB_U32       = 5 + S_ADD_U32,
  S_MIN_I32        = 6 + S_ADD_U32,
  S_MIN_U32        = 7 + S_ADD_U32,
  S_MAX_I32        = 8 + S_ADD_U32,
  S_MAX_U32        = 9 + S_ADD_U32,
  S_CSELECT_B32    = 10 + S_ADD_U32,
  S_CSELECT_B64    = 11 + S_ADD_U32,
  S_AND_B32        = 14 + S_ADD_U32,
  S_AND_B64        = 15 + S_ADD_U32,
  S_OR_B32         = 16 + S_ADD_U32,
  S_OR_B64         = 17 + S_ADD_U32,
  S_XOR_B32        = 18 + S_ADD_U32,
  S_XOR_B64        = 19 + S_ADD_U32,
  S_ANDN2_B32      = 20 + S_ADD_U32,
  S_ANDN2_B64      = 21 + S_ADD_U32,
  S_ORN2_B32       = 22 + S_ADD_U32,
  S_ORN2_B64       = 23 + S_ADD_U32,
  S_NAND_B32       = 24 + S_ADD_U32,
  S_NAND_B64       = 25 + S_ADD_U32,
  S_NOR_B32        = 26 + S_ADD_U32,
  S_NOR_B64        = 27 + S_ADD_U32,
  S_XNOR_B32       = 28 + S_ADD_U32,
  S_XNOR_B64       = 29 + S_ADD_U32,
  S_LSHL_B32       = 30 + S_ADD_U32,
  S_LSHL_B64       = 31 + S_ADD_U32,
  S_LSHR_B32       = 32 + S_ADD_U32,
  S_LSHR_B64       = 33 + S_ADD_U32,
  S_ASHR_I32       = 34 + S_ADD_U32,
  S_ASHR_I64       = 35 + S_ADD_U32,
  S_BFM_B32        = 36 + S_ADD_U32,
  S_BFM_B64        = 37 + S_ADD_U32,
  S_MUL_I32        = 38 + S_ADD_U32,
  S_BFE_U32        = 39 + S_ADD_U32,
  S_BFE_I32        = 40 + S_ADD_U32,
  S_BFE_U64        = 41 + S_ADD_U32,
  S_BFE_I64        = 42 + S_ADD_U32,
  S_CBRANCH_G_FORK = 43 + S_ADD_U32,
  S_ABSDIFF_I32    = 44 + S_ADD_U32,

  // SOPC
  S_CMP_EQ_I32,
  S_CMP_LG_I32  = 1 + S_CMP_EQ_I32,
  S_CMP_GT_I32  = 2 + S_CMP_EQ_I32,
  S_CMP_GE_I32  = 3 + S_CMP_EQ_I32,
  S_CMP_LT_I32  = 4 + S_CMP_EQ_I32,
  S_CMP_LE_I32  = 5 + S_CMP_EQ_I32,
  S_CMP_EQ_U32  = 6 + S_CMP_EQ_I32,
  S_CMP_LG_U32  = 7 + S_CMP_EQ_I32,
  S_CMP_GT_U32  = 8 + S_CMP_EQ_I32,
  S_CMP_GE_U32  = 9 + S_CMP_EQ_I32,
  S_CMP_LT_U32  = 10 + S_CMP_EQ_I32,
  S_CMP_LE_U32  = 11 + S_CMP_EQ_I32,
  S_BITCMP0_B32 = 12 + S_CMP_EQ_I32,
  S_BITCMP1_B32 = 13 + S_CMP_EQ_I32,
  S_BITCMP0_B64 = 14 + S_CMP_EQ_I32,
  S_BITCMP1_B64 = 15 + S_CMP_EQ_I32,
  S_SETVSKIP    = 16 + S_CMP_EQ_I32,

  // SOPK
  S_MOVK_I32,
  S_CMOVK_I32        = 2 + S_MOVK_I32,
  S_CMPK_EQ_I32      = 3 + S_MOVK_I32,
  S_CMPK_LG_I32      = 4 + S_MOVK_I32,
  S_CMPK_GT_I32      = 5 + S_MOVK_I32,
  S_CMPK_GE_I32      = 6 + S_MOVK_I32,
  S_CMPK_LT_I32      = 7 + S_MOVK_I32,
  S_CMPK_LE_I32      = 8 + S_MOVK_I32,
  S_CMPK_EQ_U32      = 9 + S_MOVK_I32,
  S_CMPK_LG_U32      = 10 + S_MOVK_I32,
  S_CMPK_GT_U32      = 11 + S_MOVK_I32,
  S_CMPK_GE_U32      = 12 + S_MOVK_I32,
  S_CMPK_LT_U32      = 13 + S_MOVK_I32,
  S_CMPK_LE_U32      = 14 + S_MOVK_I32,
  S_ADDK_I32         = 15 + S_MOVK_I32,
  S_MULK_I32         = 16 + S_MOVK_I32,
  S_CBRANCH_I_FORK   = 17 + S_MOVK_I32,
  S_GETREG_B32       = 18 + S_MOVK_I32,
  S_SETREG_B32       = 19 + S_MOVK_I32,
  S_GETREG_REGRD_B32 = 20 + S_MOVK_I32,
  S_SETREG_IMM32_B32 = 21 + S_MOVK_I32,

  // SOPP
  S_NOP,
  S_ENDPGM                   = 1 + S_NOP,
  S_BRANCH                   = 2 + S_NOP,
  S_CBRANCH_SCC0             = 4 + S_NOP,
  S_CBRANCH_SCC1             = 5 + S_NOP,
  S_CBRANCH_VCCZ             = 6 + S_NOP,
  S_CBRANCH_VCCNZ            = 7 + S_NOP,
  S_CBRANCH_EXECZ            = 8 + S_NOP,
  S_CBRANCH_EXECNZ           = 9 + S_NOP,
  S_BARRIER                  = 10 + S_NOP,
  S_SETKILL                  = 11 + S_NOP,
  S_WAITCNT                  = 12 + S_NOP,
  S_SETHALT                  = 13 + S_NOP,
  S_SLEEP                    = 14 + S_NOP,
  S_SETPRIO                  = 15 + S_NOP,
  S_SENDMSG                  = 16 + S_NOP,
  S_SENDMSGHALT              = 17 + S_NOP,
  S_TRAP                     = 18 + S_NOP,
  S_ICACHE_INV               = 19 + S_NOP,
  S_INCPERFLEVEL             = 20 + S_NOP,
  S_DECPERFLEVEL             = 21 + S_NOP,
  S_TTRACEDATA               = 22 + S_NOP,
  S_CBRANCH_CDBGSYS          = 23 + S_NOP,
  S_CBRANCH_CDBGUSER         = 24 + S_NOP,
  S_CBRANCH_CDBGSYS_OR_USER  = 25 + S_NOP,
  S_CBRANCH_CDBGSYS_AND_USER = 26 + S_NOP,

  // VINTRP
  V_INTERP_P1_F32,
  V_INTERP_P2_F32  = 1 + V_INTERP_P1_F32,
  V_INTERP_MOV_F32 = 2 + V_INTERP_P1_F32,

  // VOP1
  V_NOP,
  V_MOV_B32           = 1 + V_NOP,
  V_READFIRSTLANE_B32 = 2 + V_NOP,
  V_CVT_I32_F64       = 3 + V_NOP,
  V_CVT_F64_I32       = 4 + V_NOP,
  V_CVT_F32_I32       = 5 + V_NOP,
  V_CVT_F32_U32       = 6 + V_NOP,
  V_CVT_U32_F32       = 7 + V_NOP,
  V_CVT_I32_F32       = 8 + V_NOP,
  V_MOV_FED_B32       = 9 + V_NOP,
  V_CVT_F16_F32       = 10 + V_NOP,
  V_CVT_F32_F16       = 11 + V_NOP,
  V_CVT_RPI_I32_F32   = 12 + V_NOP,
  V_CVT_FLR_I32_F32   = 13 + V_NOP,
  V_CVT_OFF_F32_I4    = 14 + V_NOP,
  V_CVT_F32_F64       = 15 + V_NOP,
  V_CVT_F64_F32       = 16 + V_NOP,
  V_CVT_F32_UBYTE0    = 17 + V_NOP,
  V_CVT_F32_UBYTE1    = 18 + V_NOP,
  V_CVT_F32_UBYTE2    = 19 + V_NOP,
  V_CVT_F32_UBYTE3    = 20 + V_NOP,
  V_CVT_U32_F64       = 21 + V_NOP,
  V_CVT_F64_U32       = 22 + V_NOP,
  V_TRUNC_F64         = 23 + V_NOP,
  V_CEIL_F64          = 24 + V_NOP,
  V_RNDNE_F64         = 25 + V_NOP,
  V_FLOOR_F64         = 26 + V_NOP,
  V_FRACT_F32         = 32 + V_NOP,
  V_TRUNC_F32         = 33 + V_NOP,
  V_CEIL_F32          = 34 + V_NOP,
  V_RNDNE_F32         = 35 + V_NOP,
  V_FLOOR_F32         = 36 + V_NOP,
  V_EXP_F32           = 37 + V_NOP,
  V_LOG_CLAMP_F32     = 38 + V_NOP,
  V_LOG_F32           = 39 + V_NOP,
  V_RCP_CLAMP_F32     = 40 + V_NOP,
  V_RCP_LEGACY_F32    = 41 + V_NOP,
  V_RCP_F32           = 42 + V_NOP,
  V_RCP_IFLAG_F32     = 43 + V_NOP,
  V_RSQ_CLAMP_F32     = 44 + V_NOP,
  V_RSQ_LEGACY_F32    = 45 + V_NOP,
  V_RSQ_F32           = 46 + V_NOP,
  V_RCP_F64           = 47 + V_NOP,
  V_RCP_CLAMP_F64     = 48 + V_NOP,
  V_RSQ_F64           = 49 + V_NOP,
  V_RSQ_CLAMP_F64     = 50 + V_NOP,
  V_SQRT_F32          = 51 + V_NOP,
  V_SQRT_F64          = 52 + V_NOP,
  V_SIN_F32           = 53 + V_NOP,
  V_COS_F32           = 54 + V_NOP,
  V_NOT_B32           = 55 + V_NOP,
  V_BFREV_B32         = 56 + V_NOP,
  V_FFBH_U32          = 57 + V_NOP,
  V_FFBL_B32          = 58 + V_NOP,
  V_FFBH_I32          = 59 + V_NOP,
  V_FREXP_EXP_I32_F64 = 60 + V_NOP,
  V_FREXP_MANT_F64    = 61 + V_NOP,
  V_FRACT_F64         = 62 + V_NOP,
  V_FREXP_EXP_I32_F32 = 63 + V_NOP,
  V_FREXP_MANT_F32    = 64 + V_NOP,
  V_CLREXCP           = 65 + V_NOP,
  V_MOVRELD_B32       = 66 + V_NOP,
  V_MOVRELS_B32       = 67 + V_NOP,
  V_MOVRELSD_B32      = 68 + V_NOP,
  V_LOG_LEGACY_F32    = 69 + V_NOP,
  V_EXP_LEGACY_F32    = 70 + V_NOP,

  // VOP2
  V_CNDMASK_B32,
  V_READLANE_B32       = 1 + V_CNDMASK_B32,
  V_WRITELANE_B32      = 2 + V_CNDMASK_B32,
  V_ADD_F32            = 3 + V_CNDMASK_B32,
  V_SUB_F32            = 4 + V_CNDMASK_B32,
  V_SUBREV_F32         = 5 + V_CNDMASK_B32,
  V_MAC_LEGACY_F32     = 6 + V_CNDMASK_B32,
  V_MUL_LEGACY_F32     = 7 + V_CNDMASK_B32,
  V_MUL_F32            = 8 + V_CNDMASK_B32,
  V_MUL_I32_I24        = 9 + V_CNDMASK_B32,
  V_MUL_HI_I32_I24     = 10 + V_CNDMASK_B32,
  V_MUL_U32_U24        = 11 + V_CNDMASK_B32,
  V_MUL_HI_U32_U24     = 12 + V_CNDMASK_B32,
  V_MIN_LEGACY_F32     = 13 + V_CNDMASK_B32,
  V_MAX_LEGACY_F32     = 14 + V_CNDMASK_B32,
  V_MIN_F32            = 15 + V_CNDMASK_B32,
  V_MAX_F32            = 16 + V_CNDMASK_B32,
  V_MIN_I32            = 17 + V_CNDMASK_B32,
  V_MAX_I32            = 18 + V_CNDMASK_B32,
  V_MIN_U32            = 19 + V_CNDMASK_B32,
  V_MAX_U32            = 20 + V_CNDMASK_B32,
  V_LSHR_B32           = 21 + V_CNDMASK_B32,
  V_LSHRREV_B32        = 22 + V_CNDMASK_B32,
  V_ASHR_I32           = 23 + V_CNDMASK_B32,
  V_ASHRREV_I32        = 24 + V_CNDMASK_B32,
  V_LSHL_B32           = 25 + V_CNDMASK_B32,
  V_LSHLREV_B32        = 26 + V_CNDMASK_B32,
  V_AND_B32            = 27 + V_CNDMASK_B32,
  V_OR_B32             = 28 + V_CNDMASK_B32,
  V_XOR_B32            = 29 + V_CNDMASK_B32,
  V_BFM_B32            = 30 + V_CNDMASK_B32,
  V_MAC_F32            = 31 + V_CNDMASK_B32,
  V_MADMK_F32          = 32 + V_CNDMASK_B32,
  V_MADAK_F32          = 33 + V_CNDMASK_B32,
  V_BCNT_U32_B32       = 34 + V_CNDMASK_B32,
  V_MBCNT_LO_U32_B32   = 35 + V_CNDMASK_B32,
  V_MBCNT_HI_U32_B32   = 36 + V_CNDMASK_B32,
  V_ADD_I32            = 37 + V_CNDMASK_B32,
  V_SUB_I32            = 38 + V_CNDMASK_B32,
  V_SUBREV_I32         = 39 + V_CNDMASK_B32,
  V_ADDC_U32           = 40 + V_CNDMASK_B32,
  V_SUBB_U32           = 41 + V_CNDMASK_B32,
  V_SUBBREV_U32        = 42 + V_CNDMASK_B32,
  V_LDEXP_F32          = 43 + V_CNDMASK_B32,
  V_CVT_PKACCUM_U8_F32 = 44 + V_CNDMASK_B32,
  V_CVT_PKNORM_I16_F32 = 45 + V_CNDMASK_B32,
  V_CVT_PKNORM_U16_F32 = 46 + V_CNDMASK_B32,
  V_CVT_PKRTZ_F16_F32  = 47 + V_CNDMASK_B32,
  V_CVT_PK_U16_U32     = 48 + V_CNDMASK_B32,
  V_CVT_PK_I16_I32     = 49 + V_CNDMASK_B32,

  // VOP3
  V_DUMMY_VOP3,
  V_MAD_LEGACY_F32 = 320 + V_DUMMY_VOP3,
  V_MAD_F32        = 321 + V_DUMMY_VOP3,
  V_MAD_I32_I24    = 322 + V_DUMMY_VOP3,
  V_MAD_U32_U24    = 323 + V_DUMMY_VOP3,
  V_CUBEID_F32     = 324 + V_DUMMY_VOP3,
  V_CUBESC_F32     = 325 + V_DUMMY_VOP3,
  V_CUBETC_F32     = 326 + V_DUMMY_VOP3,
  V_CUBEMA_F32     = 327 + V_DUMMY_VOP3,
  V_BFE_U32        = 328 + V_DUMMY_VOP3,
  V_BFE_I32        = 329 + V_DUMMY_VOP3,
  V_BFI_B32        = 330 + V_DUMMY_VOP3,
  V_FMA_F32        = 331 + V_DUMMY_VOP3,
  V_FMA_F64        = 332 + V_DUMMY_VOP3,
  V_LERP_U8        = 333 + V_DUMMY_VOP3,
  V_ALIGNBIT_B32   = 334 + V_DUMMY_VOP3,
  V_ALIGNBYTE_B32  = 335 + V_DUMMY_VOP3,
  V_MULLIT_F32     = 336 + V_DUMMY_VOP3,
  V_MIN3_F32       = 337 + V_DUMMY_VOP3,
  V_MIN3_I32       = 338 + V_DUMMY_VOP3,
  V_MIN3_U32       = 339 + V_DUMMY_VOP3,
  V_MAX3_F32       = 340 + V_DUMMY_VOP3,
  V_MAX3_I32       = 341 + V_DUMMY_VOP3,
  V_MAX3_U32       = 342 + V_DUMMY_VOP3,
  V_MED3_F32       = 343 + V_DUMMY_VOP3,
  V_MED3_I32       = 344 + V_DUMMY_VOP3,
  V_MED3_U32       = 345 + V_DUMMY_VOP3,
  V_SAD_U8         = 346 + V_DUMMY_VOP3,
  V_SAD_HI_U8      = 347 + V_DUMMY_VOP3,
  V_SAD_U16        = 348 + V_DUMMY_VOP3,
  V_SAD_U32        = 349 + V_DUMMY_VOP3,
  V_CVT_PK_U8_F32  = 350 + V_DUMMY_VOP3,
  V_DIV_FIXUP_F32  = 351 + V_DUMMY_VOP3,
  V_DIV_FIXUP_F64  = 352 + V_DUMMY_VOP3,
  V_LSHL_B64       = 353 + V_DUMMY_VOP3,
  V_LSHR_B64       = 354 + V_DUMMY_VOP3,
  V_ASHR_I64       = 355 + V_DUMMY_VOP3,
  V_ADD_F64        = 356 + V_DUMMY_VOP3,
  V_MUL_F64        = 357 + V_DUMMY_VOP3,
  V_MIN_F64        = 358 + V_DUMMY_VOP3,
  V_MAX_F64        = 359 + V_DUMMY_VOP3,
  V_LDEXP_F64      = 360 + V_DUMMY_VOP3,
  V_MUL_LO_U32     = 361 + V_DUMMY_VOP3,
  V_MUL_HI_U32     = 362 + V_DUMMY_VOP3,
  V_MUL_LO_I32     = 363 + V_DUMMY_VOP3,
  V_MUL_HI_I32     = 364 + V_DUMMY_VOP3,
  V_DIV_SCALE_F32  = 365 + V_DUMMY_VOP3,
  V_DIV_SCALE_F64  = 366 + V_DUMMY_VOP3,
  V_DIV_FMAS_F32   = 367 + V_DUMMY_VOP3,
  V_DIV_FMAS_F64   = 368 + V_DUMMY_VOP3,
  V_MSAD_U8        = 369 + V_DUMMY_VOP3,
  V_QSAD_U8        = 370 + V_DUMMY_VOP3,
  V_MQSAD_U8       = 371 + V_DUMMY_VOP3,
  V_TRIG_PREOP_F64 = 372 + V_DUMMY_VOP3,
  V_MQSAD_U32_U8   = 373 + V_DUMMY_VOP3,
  V_MAD_U64_U32    = 374 + V_DUMMY_VOP3,
  V_MAD_I64_I32    = 375 + V_DUMMY_VOP3,

  // VOPC
  V_CMP_F_F32,
  V_CMP_LT_F32    = 1 + V_CMP_F_F32,
  V_CMP_EQ_F32    = 2 + V_CMP_F_F32,
  V_CMP_LE_F32    = 3 + V_CMP_F_F32,
  V_CMP_GT_F32    = 4 + V_CMP_F_F32,
  V_CMP_LG_F32    = 5 + V_CMP_F_F32,
  V_CMP_GE_F32    = 6 + V_CMP_F_F32,
  V_CMP_O_F32     = 7 + V_CMP_F_F32,
  V_CMP_U_F32     = 8 + V_CMP_F_F32,
  V_CMP_NGE_F32   = 9 + V_CMP_F_F32,
  V_CMP_NLG_F32   = 10 + V_CMP_F_F32,
  V_CMP_NGT_F32   = 11 + V_CMP_F_F32,
  V_CMP_NLE_F32   = 12 + V_CMP_F_F32,
  V_CMP_NEQ_F32   = 13 + V_CMP_F_F32,
  V_CMP_NLT_F32   = 14 + V_CMP_F_F32,
  V_CMP_T_F32     = 15 + V_CMP_F_F32,
  V_CMPX_F_F32    = 16 + V_CMP_F_F32,
  V_CMPX_LT_F32   = 17 + V_CMP_F_F32,
  V_CMPX_EQ_F32   = 18 + V_CMP_F_F32,
  V_CMPX_LE_F32   = 19 + V_CMP_F_F32,
  V_CMPX_GT_F32   = 20 + V_CMP_F_F32,
  V_CMPX_LG_F32   = 21 + V_CMP_F_F32,
  V_CMPX_GE_F32   = 22 + V_CMP_F_F32,
  V_CMPX_O_F32    = 23 + V_CMP_F_F32,
  V_CMPX_U_F32    = 24 + V_CMP_F_F32,
  V_CMPX_NGE_F32  = 25 + V_CMP_F_F32,
  V_CMPX_NLG_F32  = 26 + V_CMP_F_F32,
  V_CMPX_NGT_F32  = 27 + V_CMP_F_F32,
  V_CMPX_NLE_F32  = 28 + V_CMP_F_F32,
  V_CMPX_NEQ_F32  = 29 + V_CMP_F_F32,
  V_CMPX_NLT_F32  = 30 + V_CMP_F_F32,
  V_CMPX_T_F32    = 31 + V_CMP_F_F32,
  V_CMP_F_F64     = 32 + V_CMP_F_F32,
  V_CMP_LT_F64    = 33 + V_CMP_F_F32,
  V_CMP_EQ_F64    = 34 + V_CMP_F_F32,
  V_CMP_LE_F64    = 35 + V_CMP_F_F32,
  V_CMP_GT_F64    = 36 + V_CMP_F_F32,
  V_CMP_LG_F64    = 37 + V_CMP_F_F32,
  V_CMP_GE_F64    = 38 + V_CMP_F_F32,
  V_CMP_O_F64     = 39 + V_CMP_F_F32,
  V_CMP_U_F64     = 40 + V_CMP_F_F32,
  V_CMP_NGE_F64   = 41 + V_CMP_F_F32,
  V_CMP_NLG_F64   = 42 + V_CMP_F_F32,
  V_CMP_NGT_F64   = 43 + V_CMP_F_F32,
  V_CMP_NLE_F64   = 44 + V_CMP_F_F32,
  V_CMP_NEQ_F64   = 45 + V_CMP_F_F32,
  V_CMP_NLT_F64   = 46 + V_CMP_F_F32,
  V_CMP_T_F64     = 47 + V_CMP_F_F32,
  V_CMPX_F_F64    = 48 + V_CMP_F_F32,
  V_CMPX_LT_F64   = 49 + V_CMP_F_F32,
  V_CMPX_EQ_F64   = 50 + V_CMP_F_F32,
  V_CMPX_LE_F64   = 51 + V_CMP_F_F32,
  V_CMPX_GT_F64   = 52 + V_CMP_F_F32,
  V_CMPX_LG_F64   = 53 + V_CMP_F_F32,
  V_CMPX_GE_F64   = 54 + V_CMP_F_F32,
  V_CMPX_O_F64    = 55 + V_CMP_F_F32,
  V_CMPX_U_F64    = 56 + V_CMP_F_F32,
  V_CMPX_NGE_F64  = 57 + V_CMP_F_F32,
  V_CMPX_NLG_F64  = 58 + V_CMP_F_F32,
  V_CMPX_NGT_F64  = 59 + V_CMP_F_F32,
  V_CMPX_NLE_F64  = 60 + V_CMP_F_F32,
  V_CMPX_NEQ_F64  = 61 + V_CMP_F_F32,
  V_CMPX_NLT_F64  = 62 + V_CMP_F_F32,
  V_CMPX_T_F64    = 63 + V_CMP_F_F32,
  V_CMPS_F_F32    = 64 + V_CMP_F_F32,
  V_CMPS_LT_F32   = 65 + V_CMP_F_F32,
  V_CMPS_EQ_F32   = 66 + V_CMP_F_F32,
  V_CMPS_LE_F32   = 67 + V_CMP_F_F32,
  V_CMPS_GT_F32   = 68 + V_CMP_F_F32,
  V_CMPS_LG_F32   = 69 + V_CMP_F_F32,
  V_CMPS_GE_F32   = 70 + V_CMP_F_F32,
  V_CMPS_O_F32    = 71 + V_CMP_F_F32,
  V_CMPS_U_F32    = 72 + V_CMP_F_F32,
  V_CMPS_NGE_F32  = 73 + V_CMP_F_F32,
  V_CMPS_NLG_F32  = 74 + V_CMP_F_F32,
  V_CMPS_NGT_F32  = 75 + V_CMP_F_F32,
  V_CMPS_NLE_F32  = 76 + V_CMP_F_F32,
  V_CMPS_NEQ_F32  = 77 + V_CMP_F_F32,
  V_CMPS_NLT_F32  = 78 + V_CMP_F_F32,
  V_CMPS_T_F32    = 79 + V_CMP_F_F32,
  V_CMPSX_F_F32   = 80 + V_CMP_F_F32,
  V_CMPSX_LT_F32  = 81 + V_CMP_F_F32,
  V_CMPSX_EQ_F32  = 82 + V_CMP_F_F32,
  V_CMPSX_LE_F32  = 83 + V_CMP_F_F32,
  V_CMPSX_GT_F32  = 84 + V_CMP_F_F32,
  V_CMPSX_LG_F32  = 85 + V_CMP_F_F32,
  V_CMPSX_GE_F32  = 86 + V_CMP_F_F32,
  V_CMPSX_O_F32   = 87 + V_CMP_F_F32,
  V_CMPSX_U_F32   = 88 + V_CMP_F_F32,
  V_CMPSX_NGE_F32 = 89 + V_CMP_F_F32,
  V_CMPSX_NLG_F32 = 90 + V_CMP_F_F32,
  V_CMPSX_NGT_F32 = 91 + V_CMP_F_F32,
  V_CMPSX_NLE_F32 = 92 + V_CMP_F_F32,
  V_CMPSX_NEQ_F32 = 93 + V_CMP_F_F32,
  V_CMPSX_NLT_F32 = 94 + V_CMP_F_F32,
  V_CMPSX_T_F32   = 95 + V_CMP_F_F32,
  V_CMPS_F_F64    = 96 + V_CMP_F_F32,
  V_CMPS_LT_F64   = 97 + V_CMP_F_F32,
  V_CMPS_EQ_F64   = 98 + V_CMP_F_F32,
  V_CMPS_LE_F64   = 99 + V_CMP_F_F32,
  V_CMPS_GT_F64   = 100 + V_CMP_F_F32,
  V_CMPS_LG_F64   = 101 + V_CMP_F_F32,
  V_CMPS_GE_F64   = 102 + V_CMP_F_F32,
  V_CMPS_O_F64    = 103 + V_CMP_F_F32,
  V_CMPS_U_F64    = 104 + V_CMP_F_F32,
  V_CMPS_NGE_F64  = 105 + V_CMP_F_F32,
  V_CMPS_NLG_F64  = 106 + V_CMP_F_F32,
  V_CMPS_NGT_F64  = 107 + V_CMP_F_F32,
  V_CMPS_NLE_F64  = 108 + V_CMP_F_F32,
  V_CMPS_NEQ_F64  = 109 + V_CMP_F_F32,
  V_CMPS_NLT_F64  = 110 + V_CMP_F_F32,
  V_CMPS_T_F64    = 111 + V_CMP_F_F32,
  V_CMPSX_F_F64   = 112 + V_CMP_F_F32,
  V_CMPSX_LT_F64  = 113 + V_CMP_F_F32,
  V_CMPSX_EQ_F64  = 114 + V_CMP_F_F32,
  V_CMPSX_LE_F64  = 115 + V_CMP_F_F32,
  V_CMPSX_GT_F64  = 116 + V_CMP_F_F32,
  V_CMPSX_LG_F64  = 117 + V_CMP_F_F32,
  V_CMPSX_GE_F64  = 118 + V_CMP_F_F32,
  V_CMPSX_O_F64   = 119 + V_CMP_F_F32,
  V_CMPSX_U_F64   = 120 + V_CMP_F_F32,
  V_CMPSX_NGE_F64 = 121 + V_CMP_F_F32,
  V_CMPSX_NLG_F64 = 122 + V_CMP_F_F32,
  V_CMPSX_NGT_F64 = 123 + V_CMP_F_F32,
  V_CMPSX_NLE_F64 = 124 + V_CMP_F_F32,
  V_CMPSX_NEQ_F64 = 125 + V_CMP_F_F32,
  V_CMPSX_NLT_F64 = 126 + V_CMP_F_F32,
  V_CMPSX_T_F64   = 127 + V_CMP_F_F32,
  V_CMP_F_I32     = 128 + V_CMP_F_F32,
  V_CMP_LT_I32    = 129 + V_CMP_F_F32,
  V_CMP_EQ_I32    = 130 + V_CMP_F_F32,
  V_CMP_LE_I32    = 131 + V_CMP_F_F32,
  V_CMP_GT_I32    = 132 + V_CMP_F_F32,
  V_CMP_NE_I32    = 133 + V_CMP_F_F32,
  V_CMP_GE_I32    = 134 + V_CMP_F_F32,
  V_CMP_T_I32     = 135 + V_CMP_F_F32,
  V_CMP_CLASS_F32 = 136 + V_CMP_F_F32,

  V_CMPX_F_I32     = 144 + V_CMP_F_F32,
  V_CMPX_LT_I32    = 145 + V_CMP_F_F32,
  V_CMPX_EQ_I32    = 146 + V_CMP_F_F32,
  V_CMPX_LE_I32    = 147 + V_CMP_F_F32,
  V_CMPX_GT_I32    = 148 + V_CMP_F_F32,
  V_CMPX_NE_I32    = 149 + V_CMP_F_F32,
  V_CMPX_GE_I32    = 150 + V_CMP_F_F32,
  V_CMPX_T_I32     = 151 + V_CMP_F_F32,
  V_CMPX_CLASS_F32 = 152 + V_CMP_F_F32,
  V_CMP_F_I64      = 160 + V_CMP_F_F32,
  V_CMP_LT_I64     = 161 + V_CMP_F_F32,
  V_CMP_EQ_I64     = 162 + V_CMP_F_F32,
  V_CMP_LE_I64     = 163 + V_CMP_F_F32,
  V_CMP_GT_I64     = 164 + V_CMP_F_F32,
  V_CMP_NE_I64     = 165 + V_CMP_F_F32,
  V_CMP_GE_I64     = 166 + V_CMP_F_F32,
  V_CMP_T_I64      = 167 + V_CMP_F_F32,
  V_CMP_CLASS_F64  = 168 + V_CMP_F_F32,
  V_CMPX_F_I64     = 176 + V_CMP_F_F32,
  V_CMPX_LT_I64    = 177 + V_CMP_F_F32,
  V_CMPX_EQ_I64    = 178 + V_CMP_F_F32,
  V_CMPX_LE_I64    = 179 + V_CMP_F_F32,
  V_CMPX_GT_I64    = 180 + V_CMP_F_F32,
  V_CMPX_NE_I64    = 181 + V_CMP_F_F32,
  V_CMPX_GE_I64    = 182 + V_CMP_F_F32,
  V_CMPX_T_I64     = 183 + V_CMP_F_F32,
  V_CMPX_CLASS_F64 = 184 + V_CMP_F_F32,
  V_CMP_F_U32      = 192 + V_CMP_F_F32,
  V_CMP_LT_U32     = 193 + V_CMP_F_F32,
  V_CMP_EQ_U32     = 194 + V_CMP_F_F32,
  V_CMP_LE_U32     = 195 + V_CMP_F_F32,
  V_CMP_GT_U32     = 196 + V_CMP_F_F32,
  V_CMP_NE_U32     = 197 + V_CMP_F_F32,
  V_CMP_GE_U32     = 198 + V_CMP_F_F32,
  V_CMP_T_U32      = 199 + V_CMP_F_F32,
  V_CMPX_F_U32     = 208 + V_CMP_F_F32,
  V_CMPX_LT_U32    = 209 + V_CMP_F_F32,
  V_CMPX_EQ_U32    = 210 + V_CMP_F_F32,
  V_CMPX_LE_U32    = 211 + V_CMP_F_F32,
  V_CMPX_GT_U32    = 212 + V_CMP_F_F32,
  V_CMPX_NE_U32    = 213 + V_CMP_F_F32,
  V_CMPX_GE_U32    = 214 + V_CMP_F_F32,
  V_CMPX_T_U32     = 215 + V_CMP_F_F32,
  V_CMP_F_U64      = 224 + V_CMP_F_F32,
  V_CMP_LT_U64     = 225 + V_CMP_F_F32,
  V_CMP_EQ_U64     = 226 + V_CMP_F_F32,
  V_CMP_LE_U64     = 227 + V_CMP_F_F32,
  V_CMP_GT_U64     = 228 + V_CMP_F_F32,
  V_CMP_NE_U64     = 229 + V_CMP_F_F32,
  V_CMP_GE_U64     = 230 + V_CMP_F_F32,
  V_CMP_T_U64      = 231 + V_CMP_F_F32,
  V_CMPX_F_U64     = 240 + V_CMP_F_F32,
  V_CMPX_LT_U64    = 241 + V_CMP_F_F32,
  V_CMPX_EQ_U64    = 242 + V_CMP_F_F32,
  V_CMPX_LE_U64    = 243 + V_CMP_F_F32,
  V_CMPX_GT_U64    = 244 + V_CMP_F_F32,
  V_CMPX_NE_U64    = 245 + V_CMP_F_F32,
  V_CMPX_GE_U64    = 246 + V_CMP_F_F32,
  V_CMPX_T_U64     = 247 + V_CMP_F_F32,
  MAX_ITEM,
};

constexpr inline InstructionKind_t conv(eOpcode&& code) {
  return (InstructionKind_t)code;
}
} // namespace compiler::frontend::parser